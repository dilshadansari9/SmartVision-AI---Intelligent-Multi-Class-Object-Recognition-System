{"cells":[{"cell_type":"code","execution_count":1,"id":"98676d9f","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1562,"status":"ok","timestamp":1769607345970,"user":{"displayName":"Dilshad Ansari","userId":"01837237404505659019"},"user_tz":-330},"id":"98676d9f","outputId":"7c1d3267-5869-4ac7-892c-ba1f8e640434"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.5.0)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (12.1.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n","Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (22.0.0)\n","Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.4.0)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n","Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n","Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.18)\n","Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\n","Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.3)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.12.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2026.1.4)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n","Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.11)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n"]}],"source":["!pip install datasets pillow pandas tqdm"]},{"cell_type":"markdown","id":"BYxZpR9EMSjN","metadata":{"id":"BYxZpR9EMSjN"},"source":["### 25 Selected Classes with COCO Category IDs\n","\n","**Vehicles:** car(3), truck(8), bus(6), motorcycle(4), bicycle(2), airplane(5)  \n","**Person:** person(1)  \n","**Outdoor:** traffic light(10), stop sign(13), bench(15)  \n","**Animals:** dog(18), cat(17), horse(19), bird(16), cow(21), elephant(22)  \n","**Kitchen & Food:** bottle(44), cup(47), bowl(51), pizza(59), cake(61)  \n","**Furniture:** chair(62), couch(63), bed(65), potted plant(64)"]},{"cell_type":"code","execution_count":2,"id":"531afa7c","metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1769607345993,"user":{"displayName":"Dilshad Ansari","userId":"01837237404505659019"},"user_tz":-330},"id":"531afa7c"},"outputs":[],"source":["# 25 Selected Classes (CORRECT indices from detection-datasets/coco)\n","\n","SELECTED_CLASSES = {\n","    'person': 0,\n","    'bicycle': 1,\n","    'car': 2,\n","    'motorcycle': 3,\n","    'airplane': 4,\n","    'bus': 5,\n","    'train': 6,\n","    'truck': 7,\n","    'traffic light': 9,\n","    'stop sign': 11,\n","    'bench': 13,\n","    'bird': 14,\n","    'cat': 15,\n","    'dog': 16,\n","    'horse': 17,\n","    'cow': 19,\n","    'elephant': 20,\n","    'bottle': 39,\n","    'cup': 41,\n","    'bowl': 45,\n","    'pizza': 53,\n","    'cake': 55,\n","    'chair': 56,\n","    'couch': 57,\n","    'potted plant': 58,\n","    'bed': 59\n","}\n","\n","IMAGES_PER_CLASS = 100\n","BASE_DIR = \"smartvision_dataset\""]},{"cell_type":"code","execution_count":3,"id":"c73c91a8","metadata":{"executionInfo":{"elapsed":1566,"status":"ok","timestamp":1769607347561,"user":{"displayName":"Dilshad Ansari","userId":"01837237404505659019"},"user_tz":-330},"id":"c73c91a8"},"outputs":[],"source":["## STEP 1: IMPORTS AND CONFIGURATION\n","\n","import os\n","import json\n","from datasets import load_dataset\n","from PIL import Image\n","from collections import defaultdict\n","from tqdm import tqdm\n","import random"]},{"cell_type":"code","execution_count":4,"id":"91700ca9","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":212,"referenced_widgets":["b2f0be8506884840ae3cbd4fee38af22","042735d078504cea89c0c77b2ca5c059","733ab36650f2457ca7d68c14372346d1","19f42e275b10495eae2d15ae73945219","ed3b87f3d496444ea918288cc40c04ae","e78c869e2514412aaea1c2a8191de4d6","2381447071cb4aa6a9e0f14de15f35ed","a2a02d6c1759454e9e22593ec0cbf124","2fa46ba33b8b4702809f17db9bbc47c7","700717493ec34bfba3fdcf47a6516e20","52d4251088c74162a97416ffa148d833"]},"executionInfo":{"elapsed":1652,"status":"ok","timestamp":1769607349215,"user":{"displayName":"Dilshad Ansari","userId":"01837237404505659019"},"user_tz":-330},"id":"91700ca9","outputId":"3eb1f5d3-fcac-49af-f5db-2c1be422bdd1"},"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ“¥ Loading COCO dataset in STREAMING mode (no download)...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["Resolving data files:   0%|          | 0/40 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2f0be8506884840ae3cbd4fee38af22"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["âœ… Dataset loaded in streaming mode!\n"]}],"source":["## STEP 2: LOAD COCO DATASET FROM HUGGING FACE\n","\n","print(\"ğŸ“¥ Loading COCO dataset in STREAMING mode (no download)...\")\n","dataset = load_dataset(\"detection-datasets/coco\", split=\"train\", streaming=True)\n","print(\"âœ… Dataset loaded in streaming mode!\")"]},{"cell_type":"code","execution_count":5,"id":"c3056a4a","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c3056a4a","executionInfo":{"status":"ok","timestamp":1769607640317,"user_tz":-330,"elapsed":291097,"user":{"displayName":"Dilshad Ansari","userId":"01837237404505659019"}},"outputId":"ce752517-7c8b-4f36-f1af-172a48d9483c"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","ğŸ” Starting image collection from COCO dataset stream...\n","ğŸ¯ Target: 100 images per class\n","\n"]},{"output_type":"stream","name":"stderr","text":["5977it [04:51, 20.53it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","âœ… Collected 100 images for ALL classes!\n","\n","ğŸ“Š Collection Summary:\n","person          â†’ 100\n","bicycle         â†’ 100\n","car             â†’ 100\n","motorcycle      â†’ 100\n","airplane        â†’ 100\n","bus             â†’ 100\n","train           â†’ 100\n","truck           â†’ 100\n","traffic light   â†’ 100\n","stop sign       â†’ 100\n","bench           â†’ 100\n","bird            â†’ 100\n","cat             â†’ 100\n","dog             â†’ 100\n","horse           â†’ 100\n","cow             â†’ 100\n","elephant        â†’ 100\n","bottle          â†’ 100\n","cup             â†’ 100\n","bowl            â†’ 100\n","pizza           â†’ 100\n","cake            â†’ 100\n","chair           â†’ 100\n","couch           â†’ 100\n","potted plant    â†’ 100\n","bed             â†’ 100\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["print(\"\\nğŸ” Starting image collection from COCO dataset stream...\")\n","print(f\"ğŸ¯ Target: {IMAGES_PER_CLASS} images per class\\n\")\n","\n","# Storage\n","class_images = {cls: [] for cls in SELECTED_CLASSES}\n","class_counts = {cls: 0 for cls in SELECTED_CLASSES}\n","\n","images_processed = 0\n","max_iterations = 200000  # safety limit\n","\n","for sample in tqdm(dataset):\n","\n","    images_processed += 1\n","    if images_processed > max_iterations:\n","        print(\"âš ï¸ Safety limit reached!\")\n","        break\n","\n","    image = sample[\"image\"]\n","    annotations = sample[\"objects\"]\n","\n","    for label, bbox in zip(annotations[\"category\"], annotations[\"bbox\"]):\n","\n","        for class_name, class_id in SELECTED_CLASSES.items():\n","\n","            if class_counts[class_name] >= IMAGES_PER_CLASS:\n","                continue\n","\n","            if label == class_id:\n","                class_images[class_name].append({\n","                    \"image\": image,\n","                    \"bbox\": bbox,\n","                    \"label\": label\n","                })\n","                class_counts[class_name] += 1\n","\n","    # Stop when all classes have enough images\n","    if all(count >= IMAGES_PER_CLASS for count in class_counts.values()):\n","        print(\"\\nâœ… Collected 100 images for ALL classes!\")\n","        break\n","\n","print(\"\\nğŸ“Š Collection Summary:\")\n","for cls, count in class_counts.items():\n","    print(f\"{cls:15s} â†’ {count}\")\n"]},{"cell_type":"code","execution_count":6,"id":"216b344d","metadata":{"id":"216b344d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1769607640398,"user_tz":-330,"elapsed":75,"user":{"displayName":"Dilshad Ansari","userId":"01837237404505659019"}},"outputId":"b3843846-2ae4-4fb8-f286-4bb541144431"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","ğŸ“ Creating project folder structure...\n","\n","âœ… Folder structure created successfully!\n","\n","ğŸ“‚ Structure:\n","\n","smartvision_dataset/\n","â”œâ”€â”€ classification/\n","â”‚   â”œâ”€â”€ train/\n","â”‚   â”‚   â”œâ”€â”€ person/\n","â”‚   â”‚   â”œâ”€â”€ car/\n","â”‚   â”‚   â””â”€â”€ ... (25 class folders)\n","â”‚   â”œâ”€â”€ val/\n","â”‚   â”‚   â””â”€â”€ ... (25 class folders)\n","â”‚   â””â”€â”€ test/\n","â”‚       â””â”€â”€ ... (25 class folders)\n","â”‚\n","â””â”€â”€ detection/\n","    â”œâ”€â”€ images/\n","    â””â”€â”€ labels/\n","\n"]}],"source":["## STEP 4: CREATE FOLDER STRUCTURE\n","\n","print(\"\\nğŸ“ Creating project folder structure...\")\n","print()\n","\n","# Create main directory\n","os.makedirs(BASE_DIR, exist_ok=True)\n","\n","# Create subdirectories for Classification task\n","os.makedirs(f\"{BASE_DIR}/classification/train\", exist_ok=True)\n","os.makedirs(f\"{BASE_DIR}/classification/val\", exist_ok=True)\n","os.makedirs(f\"{BASE_DIR}/classification/test\", exist_ok=True)\n","\n","# Create subdirectories for Detection task\n","os.makedirs(f\"{BASE_DIR}/detection/images\", exist_ok=True)\n","os.makedirs(f\"{BASE_DIR}/detection/labels\", exist_ok=True)\n","\n","# Create class folders inside train/val/test\n","for class_name in SELECTED_CLASSES.keys():\n","    os.makedirs(f\"{BASE_DIR}/classification/train/{class_name}\", exist_ok=True)\n","    os.makedirs(f\"{BASE_DIR}/classification/val/{class_name}\", exist_ok=True)\n","    os.makedirs(f\"{BASE_DIR}/classification/test/{class_name}\", exist_ok=True)\n","\n","print(\"âœ… Folder structure created successfully!\")\n","print()\n","print(\"ğŸ“‚ Structure:\")\n","print(f\"\"\"\n","{BASE_DIR}/\n","â”œâ”€â”€ classification/\n","â”‚   â”œâ”€â”€ train/\n","â”‚   â”‚   â”œâ”€â”€ person/\n","â”‚   â”‚   â”œâ”€â”€ car/\n","â”‚   â”‚   â””â”€â”€ ... (25 class folders)\n","â”‚   â”œâ”€â”€ val/\n","â”‚   â”‚   â””â”€â”€ ... (25 class folders)\n","â”‚   â””â”€â”€ test/\n","â”‚       â””â”€â”€ ... (25 class folders)\n","â”‚\n","â””â”€â”€ detection/\n","    â”œâ”€â”€ images/\n","    â””â”€â”€ labels/\n","\"\"\")"]},{"cell_type":"code","execution_count":7,"id":"d99b88ce","metadata":{"id":"d99b88ce","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1769607640539,"user_tz":-330,"elapsed":139,"user":{"displayName":"Dilshad Ansari","userId":"01837237404505659019"}},"outputId":"e1f8a66e-9a8d-4299-d014-4d2cc7243707"},"outputs":[{"output_type":"stream","name":"stdout","text":["======================================================================\n","ğŸ”€ Preparing Train/Val/Test splits...\n","ğŸ“Š Split Ratio: 70% Train / 15% Val / 15% Test\n","======================================================================\n","\n","person              : Train= 70 | Val=15 | Test=15\n","bicycle             : Train= 70 | Val=15 | Test=15\n","car                 : Train= 70 | Val=15 | Test=15\n","motorcycle          : Train= 70 | Val=15 | Test=15\n","airplane            : Train= 70 | Val=15 | Test=15\n","bus                 : Train= 70 | Val=15 | Test=15\n","train               : Train= 70 | Val=15 | Test=15\n","truck               : Train= 70 | Val=15 | Test=15\n","traffic light       : Train= 70 | Val=15 | Test=15\n","stop sign           : Train= 70 | Val=15 | Test=15\n","bench               : Train= 70 | Val=15 | Test=15\n","bird                : Train= 70 | Val=15 | Test=15\n","cat                 : Train= 70 | Val=15 | Test=15\n","dog                 : Train= 70 | Val=15 | Test=15\n","horse               : Train= 70 | Val=15 | Test=15\n","cow                 : Train= 70 | Val=15 | Test=15\n","elephant            : Train= 70 | Val=15 | Test=15\n","bottle              : Train= 70 | Val=15 | Test=15\n","cup                 : Train= 70 | Val=15 | Test=15\n","bowl                : Train= 70 | Val=15 | Test=15\n","pizza               : Train= 70 | Val=15 | Test=15\n","cake                : Train= 70 | Val=15 | Test=15\n","chair               : Train= 70 | Val=15 | Test=15\n","couch               : Train= 70 | Val=15 | Test=15\n","potted plant        : Train= 70 | Val=15 | Test=15\n","bed                 : Train= 70 | Val=15 | Test=15\n"]}],"source":["## STEP 5: TRAIN/VAL/TEST SPLIT (70/15/15)\n","\n","print(\"=\"*70)\n","print(\"ğŸ”€ Preparing Train/Val/Test splits...\")\n","print(\"ğŸ“Š Split Ratio: 70% Train / 15% Val / 15% Test\")\n","print(\"=\"*70)\n","print()\n","\n","# Initialize metadata dictionary\n","metadata = {\n","    'total_images': 0,\n","    'classes': {},\n","    'splits': {'train': 0, 'val': 0, 'test': 0}\n","}\n","\n","# Create split dictionaries for each class\n","train_data = {}\n","val_data = {}\n","test_data = {}\n","\n","# Process each class\n","for class_name in SELECTED_CLASSES.keys():\n","\n","    all_items = class_images.get(class_name, [])\n","\n","    if not all_items:\n","        print(f\"âš ï¸ Warning: No images found for {class_name}\")\n","        continue\n","\n","    # Calculate split indices\n","    n = len(all_items)\n","    train_split = int(0.7 * n)   # 70% for training\n","    val_split = int(0.85 * n)    # 15% for validation\n","    # Remaining 15% for test\n","\n","    # Split the data\n","    train_data[class_name] = all_items[:train_split]\n","    val_data[class_name] = all_items[train_split:val_split]\n","    test_data[class_name] = all_items[val_split:]\n","\n","    # Store split info in metadata\n","    metadata['classes'][class_name] = {\n","        'train': len(train_data[class_name]),\n","        'val': len(val_data[class_name]),\n","        'test': len(test_data[class_name]),\n","        'total': len(all_items)\n","    }\n","\n","    metadata['splits']['train'] += len(train_data[class_name])\n","    metadata['splits']['val'] += len(val_data[class_name])\n","    metadata['splits']['test'] += len(test_data[class_name])\n","    metadata['total_images'] += len(all_items)\n","\n","    print(f\"{class_name:20s}: Train={len(train_data[class_name]):3d} | Val={len(val_data[class_name]):2d} | Test={len(test_data[class_name]):2d}\")"]},{"cell_type":"code","execution_count":11,"id":"48556611","metadata":{"id":"48556611","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1769608009309,"user_tz":-330,"elapsed":6436,"user":{"displayName":"Dilshad Ansari","userId":"01837237404505659019"}},"outputId":"662fb45f-ba48-43b3-ca78-f67342103704"},"outputs":[{"output_type":"stream","name":"stdout","text":["======================================================================\n","ğŸ’¾ STEP 6: SAVING IMAGES TO DISK\n","======================================================================\n","\n","ğŸ“ PART A: Saving Classification Images...\n","   Format: Cropped objects, 224x224 pixels\n","\n","ğŸ“‚ Processing TRAIN split...\n"]},{"output_type":"stream","name":"stderr","text":["  train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:04<00:00,  5.85it/s]\n"]},{"output_type":"stream","name":"stdout","text":["ğŸ“‚ Processing VAL split...\n"]},{"output_type":"stream","name":"stderr","text":["  val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:00<00:00, 26.61it/s]\n"]},{"output_type":"stream","name":"stdout","text":["ğŸ“‚ Processing TEST split...\n"]},{"output_type":"stream","name":"stderr","text":["  test: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26/26 [00:01<00:00, 25.83it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","======================================================================\n","âœ… CLASSIFICATION IMAGES SAVED!\n","======================================================================\n","ğŸ“Š Train: 1820 images\n","ğŸ“Š Val:   390 images\n","ğŸ“Š Test:  390 images\n","ğŸ“Š Total: 2600 images\n","\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["import os\n","from PIL import Image\n","from tqdm import tqdm\n","\n","print(\"=\"*70)\n","print(\"ğŸ’¾ STEP 6: SAVING IMAGES TO DISK\")\n","print(\"=\"*70)\n","print()\n","\n","# PART A: SAVE CLASSIFICATION IMAGES (FROM DETECTION DATA)\n","\n","print(\"ğŸ“ PART A: Saving Classification Images...\")\n","print(\"   Format: Cropped objects, 224x224 pixels\\n\")\n","\n","classification_stats = {'train': 0, 'val': 0, 'test': 0}\n","\n","# Process each split\n","for split_name, split_data in [('train', train_data), ('val', val_data), ('test', test_data)]:\n","\n","    print(f\"ğŸ“‚ Processing {split_name.upper()} split...\")\n","\n","    # Process each class\n","    for class_name, items in tqdm(split_data.items(), desc=f\"  {split_name}\"):\n","\n","        class_folder = f\"{BASE_DIR}/classification/{split_name}/{class_name}\"\n","        os.makedirs(class_folder, exist_ok=True)\n","\n","        # Save each image\n","        for img_idx, item in enumerate(items):\n","\n","            img = item['image']\n","            bbox = item['bbox']          # âœ… FIX\n","            class_id = item['label']     # âœ… FIX\n","\n","            try:\n","                x, y, w, h = bbox\n","\n","                # Crop and resize\n","                cropped_img = img.crop((x, y, x + w, y + h))\n","                cropped_img = cropped_img.resize((224, 224), Image.LANCZOS)\n","\n","                # Save\n","                img_filename = f\"{class_name}_{split_name}_{img_idx:04d}.jpg\"\n","                img_path = os.path.join(class_folder, img_filename)\n","                cropped_img.save(img_path, quality=95)\n","\n","                classification_stats[split_name] += 1\n","\n","            except Exception as e:\n","                print(f\"âš ï¸ Error: {class_name} image {img_idx}: {e}\")\n","\n","print()\n","print(\"=\"*70)\n","print(\"âœ… CLASSIFICATION IMAGES SAVED!\")\n","print(\"=\"*70)\n","print(f\"ğŸ“Š Train: {classification_stats['train']} images\")\n","print(f\"ğŸ“Š Val:   {classification_stats['val']} images\")\n","print(f\"ğŸ“Š Test:  {classification_stats['test']} images\")\n","print(f\"ğŸ“Š Total: {sum(classification_stats.values())} images\")\n","print()\n"]},{"cell_type":"code","execution_count":13,"id":"df35bbc9","metadata":{"id":"df35bbc9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1769608077566,"user_tz":-330,"elapsed":3270,"user":{"displayName":"Dilshad Ansari","userId":"01837237404505659019"}},"outputId":"d9818add-5c2c-4110-e4fd-270e6b1c8f23"},"outputs":[{"output_type":"stream","name":"stdout","text":["======================================================================\n","ğŸ“ PART B: Saving Detection Images & Annotations...\n","   Format: Full images with YOLO .txt labels\n","\n","ğŸ“Š Total detection images: 2210\n","\n"]},{"output_type":"stream","name":"stderr","text":["Saving detection data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2210/2210 [00:03<00:00, 676.61it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","======================================================================\n","âœ… DETECTION DATASET CREATED!\n","======================================================================\n","ğŸ“Š Images:     2210\n","ğŸ“Š Labels:     2210\n","ğŸ“Š Objects:    2210\n","ğŸ“Š Avg/image:  1.00\n","\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["# PART B: SAVE DETECTION IMAGES (YOLO FORMAT)\n","\n","print(\"=\"*70)\n","print(\"ğŸ“ PART B: Saving Detection Images & Annotations...\")\n","print(\"   Format: Full images with YOLO .txt labels\\n\")\n","\n","detection_stats = {'images': 0, 'annotations': 0, 'objects': 0}\n","\n","# COCO â†’ YOLO class mapping\n","coco_to_yolo = {class_id: idx for idx, class_id in enumerate(SELECTED_CLASSES.values())}\n","\n","# Combine train + val for detection\n","all_detection_data = []\n","for class_name in SELECTED_CLASSES.keys():\n","    all_detection_data.extend(train_data.get(class_name, []))\n","    all_detection_data.extend(val_data.get(class_name, []))\n","\n","print(f\"ğŸ“Š Total detection images: {len(all_detection_data)}\\n\")\n","\n","# Save images and YOLO labels\n","for img_idx, item in enumerate(tqdm(all_detection_data, desc=\"Saving detection data\")):\n","\n","    img = item['image']\n","    img_width, img_height = img.size\n","\n","    # Save full image\n","    img_filename = f\"image_{img_idx:06d}.jpg\"\n","    img_path = os.path.join(f\"{BASE_DIR}/detection/images\", img_filename)\n","    img.save(img_path, quality=95)\n","    detection_stats['images'] += 1\n","\n","    # âœ… FIX: use bbox + label\n","    bbox = item['bbox']\n","    cat_id = item['label']\n","\n","    # Convert to YOLO format\n","    x, y, w, h = bbox\n","    x_center = (x + w / 2) / img_width\n","    y_center = (y + h / 2) / img_height\n","    w_norm = w / img_width\n","    h_norm = h / img_height\n","\n","    yolo_class_id = coco_to_yolo[cat_id]\n","\n","    yolo_line = f\"{yolo_class_id} {x_center:.6f} {y_center:.6f} {w_norm:.6f} {h_norm:.6f}\"\n","\n","    # Save label file\n","    label_filename = f\"image_{img_idx:06d}.txt\"\n","    label_path = os.path.join(f\"{BASE_DIR}/detection/labels\", label_filename)\n","\n","    with open(label_path, 'w') as f:\n","        f.write(yolo_line)\n","\n","    detection_stats['annotations'] += 1\n","    detection_stats['objects'] += 1\n","\n","print()\n","print(\"=\"*70)\n","print(\"âœ… DETECTION DATASET CREATED!\")\n","print(\"=\"*70)\n","print(f\"ğŸ“Š Images:     {detection_stats['images']}\")\n","print(f\"ğŸ“Š Labels:     {detection_stats['annotations']}\")\n","print(f\"ğŸ“Š Objects:    {detection_stats['objects']}\")\n","print(f\"ğŸ“Š Avg/image:  {detection_stats['objects']/detection_stats['images']:.2f}\")\n","print()\n"]},{"cell_type":"code","execution_count":14,"id":"f9222e7a","metadata":{"id":"f9222e7a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1769608121186,"user_tz":-330,"elapsed":43,"user":{"displayName":"Dilshad Ansari","userId":"01837237404505659019"}},"outputId":"b8880b69-0f3e-4bce-b857-18dfbef02de0"},"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ“ Creating YOLO configuration file...\n","\n","âœ… Created: smartvision_dataset/detection/data.yaml\n","\n"]}],"source":["# PART C: CREATE YOLO CONFIG FILE\n","\n","print(\"ğŸ“ Creating YOLO configuration file...\\n\")\n","\n","yaml_content = f\"\"\"# SmartVision Dataset - YOLOv8 Configuration\n","path: {os.path.abspath(BASE_DIR)}/detection\n","train: images\n","val: images\n","\n","names:\n","  0: person\n","  1: bicycle\n","  2: car\n","  3: motorcycle\n","  4: airplane\n","  5: bus\n","  6: train\n","  7: truck\n","  8: traffic light\n","  9: stop sign\n","  10: bench\n","  11: bird\n","  12: cat\n","  13: dog\n","  14: horse\n","  15: cow\n","  16: elephant\n","  17: bottle\n","  18: cup\n","  19: bowl\n","  20: pizza\n","  21: cake\n","  22: chair\n","  23: couch\n","  24: potted plant\n","  25: bed\n","\n","nc: 26\n","\"\"\"\n","\n","yaml_path = f\"{BASE_DIR}/detection/data.yaml\"\n","with open(yaml_path, 'w') as f:\n","    f.write(yaml_content)\n","\n","print(f\"âœ… Created: {yaml_path}\\n\")"]},{"cell_type":"code","execution_count":15,"id":"6a2390e9","metadata":{"id":"6a2390e9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1769608124271,"user_tz":-330,"elapsed":7,"user":{"displayName":"Dilshad Ansari","userId":"01837237404505659019"}},"outputId":"ea398ced-e19f-467b-ee51-3d9eeefbaad0"},"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ“Š Saving metadata...\n","\n","âœ… Saved: smartvision_dataset/dataset_metadata.json\n","\n"]}],"source":["# PART D: SAVE METADATA\n","\n","print(\"ğŸ“Š Saving metadata...\\n\")\n","\n","metadata['classification'] = classification_stats\n","metadata['detection'] = detection_stats\n","metadata['dataset_path'] = os.path.abspath(BASE_DIR)\n","\n","metadata_path = f\"{BASE_DIR}/dataset_metadata.json\"\n","with open(metadata_path, 'w') as f:\n","    json.dump(metadata, indent=2, fp=f)\n","\n","print(f\"âœ… Saved: {metadata_path}\\n\")"]},{"cell_type":"code","execution_count":16,"id":"f216bcbb","metadata":{"id":"f216bcbb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1769608126727,"user_tz":-330,"elapsed":20,"user":{"displayName":"Dilshad Ansari","userId":"01837237404505659019"}},"outputId":"b0eb058f-5465-4abe-e6f2-9db9d9df7992"},"outputs":[{"output_type":"stream","name":"stdout","text":["======================================================================\n","ğŸ‰ DATASET SETUP COMPLETE!\n","======================================================================\n","\n","ğŸ“ Location: /content/smartvision_dataset\n","\n","ğŸ“‚ Classification Dataset:\n","   â”œâ”€ Train:  1820 images (70%)\n","   â”œâ”€ Val:    390 images (15%)\n","   â”œâ”€ Test:   390 images (15%)\n","   â””â”€ Total:  2600 cropped images (224x224)\n","\n","ğŸ“‚ Detection Dataset:\n","   â”œâ”€ Images: 2210 full images\n","   â”œâ”€ Labels: 2210 YOLO .txt files\n","   â””â”€ Objects: 2210 annotated objects\n","\n","======================================================================\n","âœ… LEARNERS CAN NOW START:\n","======================================================================\n","Step 7:  Exploratory Data Analysis (EDA)\n","Step 8:  Train Classification Models\n","Step 9:  Train YOLO Detection Model\n","Step 10: Build Streamlit Application\n","Step 11: Deploy to Hugging Face Spaces\n","======================================================================\n"]}],"source":["print(\"=\"*70)\n","print(\"ğŸ‰ DATASET SETUP COMPLETE!\")\n","print(\"=\"*70)\n","print()\n","print(f\"ğŸ“ Location: {os.path.abspath(BASE_DIR)}\")\n","print()\n","print(\"ğŸ“‚ Classification Dataset:\")\n","print(f\"   â”œâ”€ Train:  {classification_stats['train']} images (70%)\")\n","print(f\"   â”œâ”€ Val:    {classification_stats['val']} images (15%)\")\n","print(f\"   â”œâ”€ Test:   {classification_stats['test']} images (15%)\")\n","print(f\"   â””â”€ Total:  {sum(classification_stats.values())} cropped images (224x224)\")\n","print()\n","print(\"ğŸ“‚ Detection Dataset:\")\n","print(f\"   â”œâ”€ Images: {detection_stats['images']} full images\")\n","print(f\"   â”œâ”€ Labels: {detection_stats['annotations']} YOLO .txt files\")\n","print(f\"   â””â”€ Objects: {detection_stats['objects']} annotated objects\")\n","print()\n","print(\"=\"*70)\n","print(\"âœ… LEARNERS CAN NOW START:\")\n","print(\"=\"*70)\n","print(\"Step 7:  Exploratory Data Analysis (EDA)\")\n","print(\"Step 8:  Train Classification Models\")\n","print(\"Step 9:  Train YOLO Detection Model\")\n","print(\"Step 10: Build Streamlit Application\")\n","print(\"Step 11: Deploy to Hugging Face Spaces\")\n","print(\"=\"*70)"]},{"cell_type":"code","execution_count":null,"id":"980b4836","metadata":{"id":"980b4836","executionInfo":{"status":"aborted","timestamp":1769607640667,"user_tz":-330,"elapsed":296485,"user":{"displayName":"Dilshad Ansari","userId":"01837237404505659019"}}},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":17,"id":"KHHhLOY_5BWa","metadata":{"id":"KHHhLOY_5BWa","executionInfo":{"status":"ok","timestamp":1769608133320,"user_tz":-330,"elapsed":2,"user":{"displayName":"Dilshad Ansari","userId":"01837237404505659019"}}},"outputs":[],"source":["import os\n","\n","save_path = \"/content/drive/MyDrive/datasets/huggingface_images\"\n","os.makedirs(save_path, exist_ok=True)\n"]},{"cell_type":"code","execution_count":null,"id":"z8DkQjQQ7KOn","metadata":{"id":"z8DkQjQQ7KOn","executionInfo":{"status":"aborted","timestamp":1769607640669,"user_tz":-330,"elapsed":296487,"user":{"displayName":"Dilshad Ansari","userId":"01837237404505659019"}}},"outputs":[],"source":["!ls /content\n"]},{"cell_type":"code","execution_count":null,"id":"_lG0sYWr7dzN","metadata":{"id":"_lG0sYWr7dzN","executionInfo":{"status":"aborted","timestamp":1769607640670,"user_tz":-330,"elapsed":296487,"user":{"displayName":"Dilshad Ansari","userId":"01837237404505659019"}}},"outputs":[],"source":["!mv /content/smartvision_dataset/ /content/drive/MyDrive/"]},{"cell_type":"code","execution_count":null,"id":"1D2NPWt67pta","metadata":{"id":"1D2NPWt67pta","executionInfo":{"status":"aborted","timestamp":1769607640671,"user_tz":-330,"elapsed":296488,"user":{"displayName":"Dilshad Ansari","userId":"01837237404505659019"}}},"outputs":[],"source":[]}],"metadata":{"accelerator":"TPU","colab":{"gpuType":"V5E1","provenance":[{"file_id":"1KsjW74DSd34IGrqTIqah_jpm_Oyk43Kp","timestamp":1767019355609}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"b2f0be8506884840ae3cbd4fee38af22":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_042735d078504cea89c0c77b2ca5c059","IPY_MODEL_733ab36650f2457ca7d68c14372346d1","IPY_MODEL_19f42e275b10495eae2d15ae73945219"],"layout":"IPY_MODEL_ed3b87f3d496444ea918288cc40c04ae"}},"042735d078504cea89c0c77b2ca5c059":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e78c869e2514412aaea1c2a8191de4d6","placeholder":"â€‹","style":"IPY_MODEL_2381447071cb4aa6a9e0f14de15f35ed","value":"Resolvingâ€‡dataâ€‡files:â€‡100%"}},"733ab36650f2457ca7d68c14372346d1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a2a02d6c1759454e9e22593ec0cbf124","max":40,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2fa46ba33b8b4702809f17db9bbc47c7","value":40}},"19f42e275b10495eae2d15ae73945219":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_700717493ec34bfba3fdcf47a6516e20","placeholder":"â€‹","style":"IPY_MODEL_52d4251088c74162a97416ffa148d833","value":"â€‡40/40â€‡[00:00&lt;00:00,â€‡5201.75it/s]"}},"ed3b87f3d496444ea918288cc40c04ae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e78c869e2514412aaea1c2a8191de4d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2381447071cb4aa6a9e0f14de15f35ed":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a2a02d6c1759454e9e22593ec0cbf124":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2fa46ba33b8b4702809f17db9bbc47c7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"700717493ec34bfba3fdcf47a6516e20":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"52d4251088c74162a97416ffa148d833":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}