{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMfsaxqIUxZbSYFt/9AEXfD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vkmMOoEhzjeB","executionInfo":{"status":"ok","timestamp":1769528343854,"user_tz":-330,"elapsed":11481,"user":{"displayName":"Dilshad Ansari","userId":"01837237404505659019"}},"outputId":"69a3bd4c-4590-43a4-d583-aad9b91ab5a9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import os\n","import numpy as np\n","from tensorflow.keras.preprocessing import image\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.applications.efficientnet import preprocess_input as eff_preprocess\n","from tensorflow.keras.applications.mobilenet_v2 import preprocess_input as mob_preprocess\n","from tensorflow.keras.applications.vgg16 import preprocess_input as vgg_preprocess\n"],"metadata":{"id":"z3Wjeuj58Hnk","executionInfo":{"status":"ok","timestamp":1769528361123,"user_tz":-330,"elapsed":6415,"user":{"displayName":"Dilshad Ansari","userId":"01837237404505659019"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["subset_dir   = \"/content/drive/MyDrive/SmartVisionAI/smartvision_dataset/classification_subset\"\n","Subset=['train','test','val']\n"],"metadata":{"id":"eIVCN__Sz7EH","executionInfo":{"status":"ok","timestamp":1769528361125,"user_tz":-330,"elapsed":1,"user":{"displayName":"Dilshad Ansari","userId":"01837237404505659019"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["import os\n","\n","train_dir=os.path.join(subset_dir, \"train\")\n","val_dir=os.path.join(subset_dir, \"val\")\n","test_dir=os.path.join(subset_dir, \"test\")"],"metadata":{"id":"0cTv-o1g2WhQ","executionInfo":{"status":"ok","timestamp":1769528361126,"user_tz":-330,"elapsed":0,"user":{"displayName":"Dilshad Ansari","userId":"01837237404505659019"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["class_names = sorted(os.listdir(test_dir))\n","IMG_SIZE = (224, 224)"],"metadata":{"id":"s7KrfSE88U-y","executionInfo":{"status":"ok","timestamp":1769528362311,"user_tz":-330,"elapsed":1184,"user":{"displayName":"Dilshad Ansari","userId":"01837237404505659019"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.applications.efficientnet import preprocess_input\n","\n","model = load_model(\"/content/drive/MyDrive/SmartVisionAI/models/efficientnet_stage1.keras\")\n","\n","print(\"\\n===== Testing EfficientNet =====\")\n","\n","for actual_class in class_names:\n","    class_folder = os.path.join(test_dir, actual_class)\n","    images = os.listdir(class_folder)[:3]\n","\n","    print(f\"\\nClass: {actual_class}\")\n","\n","    for img_name in images:\n","        img_path = os.path.join(class_folder, img_name)\n","\n","        img = image.load_img(img_path, target_size=IMG_SIZE)\n","        img_array = image.img_to_array(img)\n","        img_array = np.expand_dims(img_array, axis=0)\n","        img_array = preprocess_input(img_array)\n","\n","        pred = model.predict(img_array, verbose=0)\n","        predicted_class = class_names[np.argmax(pred)]\n","        confidence = np.max(pred) * 100\n","\n","        print(f\"{img_name} â†’ {predicted_class} ({confidence:.2f}%)\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_z6FNxV69k7X","executionInfo":{"status":"ok","timestamp":1769528415610,"user_tz":-330,"elapsed":53300,"user":{"displayName":"Dilshad Ansari","userId":"01837237404505659019"}},"outputId":"3da7e6ae-59f0-41ed-fdf3-fa627d076824"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","===== Testing EfficientNet =====\n","\n","Class: airplane\n","0023.jpg â†’ airplane (100.00%)\n","0013.jpg â†’ airplane (100.00%)\n","0031.jpg â†’ airplane (100.00%)\n","\n","Class: bed\n","0042.jpg â†’ bed (99.53%)\n","0045.jpg â†’ elephant (33.26%)\n","0024.jpg â†’ bed (99.81%)\n","\n","Class: bench\n","0015.jpg â†’ bench (99.45%)\n","0013.jpg â†’ bench (74.41%)\n","0037.jpg â†’ dog (52.71%)\n","\n","Class: bicycle\n","0036.jpg â†’ bicycle (100.00%)\n","0033.jpg â†’ bicycle (21.76%)\n","0031.jpg â†’ bicycle (100.00%)\n","\n","Class: bird\n","0041.jpg â†’ bird (99.94%)\n","0028.jpg â†’ bird (99.44%)\n","0003.jpg â†’ bird (100.00%)\n","\n","Class: bottle\n","0032.jpg â†’ bottle (97.86%)\n","0006.jpg â†’ person (24.68%)\n","0022.jpg â†’ bottle (99.96%)\n","\n","Class: bowl\n","0033.jpg â†’ dog (36.05%)\n","0025.jpg â†’ bowl (96.32%)\n","0004.jpg â†’ bowl (96.15%)\n","\n","Class: bus\n","0007.jpg â†’ truck (45.81%)\n","0048.jpg â†’ bus (99.99%)\n","0025.jpg â†’ bus (60.40%)\n","\n","Class: cake\n","0001.jpg â†’ cake (100.00%)\n","0010.jpg â†’ cake (99.97%)\n","0012.jpg â†’ cake (100.00%)\n","\n","Class: car\n","0030.jpg â†’ car (29.69%)\n","0022.jpg â†’ car (46.66%)\n","0028.jpg â†’ car (99.96%)\n","\n","Class: cat\n","0042.jpg â†’ cat (100.00%)\n","0028.jpg â†’ bed (39.34%)\n","0036.jpg â†’ cat (89.03%)\n","\n","Class: chair\n","0034.jpg â†’ chair (65.62%)\n","0006.jpg â†’ cup (55.98%)\n","0029.jpg â†’ chair (99.98%)\n","\n","Class: couch\n","0028.jpg â†’ cake (47.59%)\n","0024.jpg â†’ couch (40.42%)\n","0022.jpg â†’ bird (36.40%)\n","\n","Class: cow\n","0030.jpg â†’ cow (99.97%)\n","0039.jpg â†’ cow (100.00%)\n","0040.jpg â†’ cow (99.98%)\n","\n","Class: cup\n","0018.jpg â†’ cup (98.37%)\n","0046.jpg â†’ couch (24.11%)\n","0028.jpg â†’ cup (47.16%)\n","\n","Class: dog\n","0007.jpg â†’ dog (82.80%)\n","0032.jpg â†’ cat (88.48%)\n","0001.jpg â†’ dog (99.21%)\n","\n","Class: elephant\n","0049.jpg â†’ elephant (99.96%)\n","0048.jpg â†’ elephant (99.99%)\n","0002.jpg â†’ elephant (100.00%)\n","\n","Class: horse\n","0003.jpg â†’ horse (99.88%)\n","0024.jpg â†’ horse (99.71%)\n","0037.jpg â†’ cow (57.52%)\n","\n","Class: motorcycle\n","0040.jpg â†’ motorcycle (99.61%)\n","0036.jpg â†’ motorcycle (99.99%)\n","0020.jpg â†’ motorcycle (99.99%)\n","\n","Class: person\n","0049.jpg â†’ person (98.49%)\n","0028.jpg â†’ person (33.94%)\n","0005.jpg â†’ person (100.00%)\n","\n","Class: pizza\n","0023.jpg â†’ pizza (79.06%)\n","0046.jpg â†’ pizza (100.00%)\n","0016.jpg â†’ pizza (100.00%)\n","\n","Class: potted plant\n","0028.jpg â†’ potted plant (100.00%)\n","0004.jpg â†’ potted plant (87.75%)\n","0032.jpg â†’ potted plant (98.48%)\n","\n","Class: stop sign\n","0004.jpg â†’ stop sign (57.12%)\n","0033.jpg â†’ stop sign (99.83%)\n","0045.jpg â†’ stop sign (100.00%)\n","\n","Class: traffic light\n","0023.jpg â†’ traffic light (100.00%)\n","0046.jpg â†’ traffic light (57.09%)\n","0026.jpg â†’ bottle (57.53%)\n","\n","Class: train\n","0024.jpg â†’ train (100.00%)\n","0034.jpg â†’ train (100.00%)\n","0008.jpg â†’ train (100.00%)\n","\n","Class: truck\n","0011.jpg â†’ truck (41.77%)\n","0003.jpg â†’ truck (93.29%)\n","0043.jpg â†’ airplane (21.09%)\n"]}]},{"cell_type":"code","source":["from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n","\n","model = load_model(\"/content/drive/MyDrive/SmartVisionAI/models/mobilenetv2_smartvision.keras\")\n","\n","print(\"\\n===== Testing MobileNetV2 =====\")\n","\n","for actual_class in class_names:\n","    class_folder = os.path.join(test_dir, actual_class)\n","    images = os.listdir(class_folder)[:3]\n","\n","    print(f\"\\nClass: {actual_class}\")\n","\n","    for img_name in images:\n","        img_path = os.path.join(class_folder, img_name)\n","\n","        img = image.load_img(img_path, target_size=IMG_SIZE)\n","        img_array = image.img_to_array(img)\n","        img_array = np.expand_dims(img_array, axis=0)\n","        img_array = preprocess_input(img_array)\n","\n","        pred = model.predict(img_array, verbose=0)\n","        predicted_class = class_names[np.argmax(pred)]\n","        confidence = np.max(pred) * 100\n","\n","        print(f\"{img_name} â†’ {predicted_class} ({confidence:.2f}%)\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HO6cb5yn9xJP","executionInfo":{"status":"ok","timestamp":1769528428442,"user_tz":-330,"elapsed":12830,"user":{"displayName":"Dilshad Ansari","userId":"01837237404505659019"}},"outputId":"c9e5158c-26e7-4b39-fcc1-294a010bb122"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","===== Testing MobileNetV2 =====\n","\n","Class: airplane\n","0023.jpg â†’ airplane (100.00%)\n","0013.jpg â†’ airplane (99.98%)\n","0031.jpg â†’ airplane (100.00%)\n","\n","Class: bed\n","0042.jpg â†’ bed (87.78%)\n","0045.jpg â†’ bed (28.98%)\n","0024.jpg â†’ cake (31.14%)\n","\n","Class: bench\n","0015.jpg â†’ bench (70.57%)\n","0013.jpg â†’ person (46.81%)\n","0037.jpg â†’ bench (43.65%)\n","\n","Class: bicycle\n","0036.jpg â†’ bicycle (66.65%)\n","0033.jpg â†’ potted plant (19.62%)\n","0031.jpg â†’ bicycle (97.99%)\n","\n","Class: bird\n","0041.jpg â†’ bird (94.04%)\n","0028.jpg â†’ train (55.89%)\n","0003.jpg â†’ bird (99.64%)\n","\n","Class: bottle\n","0032.jpg â†’ bottle (28.47%)\n","0006.jpg â†’ chair (27.14%)\n","0022.jpg â†’ bottle (97.45%)\n","\n","Class: bowl\n","0033.jpg â†’ bowl (20.17%)\n","0025.jpg â†’ cake (45.82%)\n","0004.jpg â†’ bowl (92.04%)\n","\n","Class: bus\n","0007.jpg â†’ traffic light (23.89%)\n","0048.jpg â†’ bus (99.87%)\n","0025.jpg â†’ bus (91.03%)\n","\n","Class: cake\n","0001.jpg â†’ cake (99.95%)\n","0010.jpg â†’ cake (96.95%)\n","0012.jpg â†’ cake (97.53%)\n","\n","Class: car\n","0030.jpg â†’ potted plant (32.87%)\n","0022.jpg â†’ person (23.44%)\n","0028.jpg â†’ car (90.81%)\n","\n","Class: cat\n","0042.jpg â†’ cat (99.77%)\n","0028.jpg â†’ bed (35.37%)\n","0036.jpg â†’ cat (91.30%)\n","\n","Class: chair\n","0034.jpg â†’ bench (42.55%)\n","0006.jpg â†’ bench (26.77%)\n","0029.jpg â†’ chair (96.79%)\n","\n","Class: couch\n","0028.jpg â†’ person (56.37%)\n","0024.jpg â†’ elephant (31.30%)\n","0022.jpg â†’ potted plant (51.59%)\n","\n","Class: cow\n","0030.jpg â†’ person (33.96%)\n","0039.jpg â†’ cow (98.88%)\n","0040.jpg â†’ horse (58.67%)\n","\n","Class: cup\n","0018.jpg â†’ cup (98.23%)\n","0046.jpg â†’ potted plant (16.79%)\n","0028.jpg â†’ pizza (67.62%)\n","\n","Class: dog\n","0007.jpg â†’ cow (79.62%)\n","0032.jpg â†’ dog (85.66%)\n","0001.jpg â†’ dog (79.02%)\n","\n","Class: elephant\n","0049.jpg â†’ elephant (97.02%)\n","0048.jpg â†’ elephant (98.16%)\n","0002.jpg â†’ elephant (94.89%)\n","\n","Class: horse\n","0003.jpg â†’ horse (82.88%)\n","0024.jpg â†’ horse (96.62%)\n","0037.jpg â†’ bench (44.31%)\n","\n","Class: motorcycle\n","0040.jpg â†’ motorcycle (100.00%)\n","0036.jpg â†’ motorcycle (98.64%)\n","0020.jpg â†’ motorcycle (99.97%)\n","\n","Class: person\n","0049.jpg â†’ person (86.78%)\n","0028.jpg â†’ motorcycle (91.28%)\n","0005.jpg â†’ person (98.75%)\n","\n","Class: pizza\n","0023.jpg â†’ train (80.25%)\n","0046.jpg â†’ pizza (99.68%)\n","0016.jpg â†’ pizza (99.98%)\n","\n","Class: potted plant\n","0028.jpg â†’ potted plant (99.83%)\n","0004.jpg â†’ potted plant (99.93%)\n","0032.jpg â†’ potted plant (85.76%)\n","\n","Class: stop sign\n","0004.jpg â†’ potted plant (18.01%)\n","0033.jpg â†’ stop sign (70.90%)\n","0045.jpg â†’ stop sign (99.99%)\n","\n","Class: traffic light\n","0023.jpg â†’ traffic light (97.00%)\n","0046.jpg â†’ traffic light (56.93%)\n","0026.jpg â†’ bottle (13.94%)\n","\n","Class: train\n","0024.jpg â†’ train (100.00%)\n","0034.jpg â†’ train (99.97%)\n","0008.jpg â†’ train (100.00%)\n","\n","Class: truck\n","0011.jpg â†’ airplane (84.76%)\n","0003.jpg â†’ stop sign (20.01%)\n","0043.jpg â†’ person (30.14%)\n"]}]},{"cell_type":"code","source":["# Load model\n","model = load_model(\"/content/drive/MyDrive/SmartVisionAI/models/vgg16_best_model.h5\")\n","\n","# ðŸ”¥ Get input size dynamically\n","input_height = model.input_shape[1]\n","input_width = model.input_shape[2]\n","IMG_SIZE = (input_height, input_width)\n","\n","print(\"VGG16 expects input size:\", IMG_SIZE)\n","\n","print(\"\\n===== Testing VGG16 =====\")\n","\n","for actual_class in class_names:\n","    class_folder = os.path.join(test_dir, actual_class)\n","    images = os.listdir(class_folder)[:3]   # only 3 images\n","\n","    print(f\"\\nClass: {actual_class}\")\n","\n","    for img_name in images:\n","        img_path = os.path.join(class_folder, img_name)\n","\n","        img = image.load_img(img_path, target_size=IMG_SIZE)\n","        img_array = image.img_to_array(img)\n","        img_array = np.expand_dims(img_array, axis=0)\n","        img_array = preprocess_input(img_array)\n","\n","        pred = model.predict(img_array, verbose=0)\n","        predicted_class = class_names[np.argmax(pred)]\n","        confidence = np.max(pred) * 100\n","\n","        print(f\"{img_name} â†’ {predicted_class} ({confidence:.2f}%)\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1N4wHbpB-4C_","executionInfo":{"status":"ok","timestamp":1769528442597,"user_tz":-330,"elapsed":14152,"user":{"displayName":"Dilshad Ansari","userId":"01837237404505659019"}},"outputId":"0507e21e-34ef-40e8-8a15-9b6205e85f73"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]},{"output_type":"stream","name":"stdout","text":["VGG16 expects input size: (160, 160)\n","\n","===== Testing VGG16 =====\n","\n","Class: airplane\n","0023.jpg â†’ airplane (100.00%)\n","0013.jpg â†’ airplane (99.96%)\n","0031.jpg â†’ airplane (99.98%)\n","\n","Class: bed\n","0042.jpg â†’ pizza (46.19%)\n","0045.jpg â†’ cat (36.07%)\n","0024.jpg â†’ pizza (49.87%)\n","\n","Class: bench\n","0015.jpg â†’ bicycle (23.54%)\n","0013.jpg â†’ couch (20.18%)\n","0037.jpg â†’ train (35.97%)\n","\n","Class: bicycle\n","0036.jpg â†’ bicycle (65.31%)\n","0033.jpg â†’ truck (10.96%)\n","0031.jpg â†’ motorcycle (44.50%)\n","\n","Class: bird\n","0041.jpg â†’ train (32.18%)\n","0028.jpg â†’ train (47.48%)\n","0003.jpg â†’ cow (57.19%)\n","\n","Class: bottle\n","0032.jpg â†’ bowl (23.76%)\n","0006.jpg â†’ motorcycle (18.45%)\n","0022.jpg â†’ traffic light (28.17%)\n","\n","Class: bowl\n","0033.jpg â†’ couch (10.25%)\n","0025.jpg â†’ bowl (71.18%)\n","0004.jpg â†’ bowl (99.99%)\n","\n","Class: bus\n","0007.jpg â†’ bicycle (46.46%)\n","0048.jpg â†’ bus (95.37%)\n","0025.jpg â†’ train (82.59%)\n","\n","Class: cake\n","0001.jpg â†’ cake (99.13%)\n","0010.jpg â†’ pizza (42.98%)\n","0012.jpg â†’ cake (97.88%)\n","\n","Class: car\n","0030.jpg â†’ car (25.69%)\n","0022.jpg â†’ truck (21.22%)\n","0028.jpg â†’ airplane (67.83%)\n","\n","Class: cat\n","0042.jpg â†’ cat (54.14%)\n","0028.jpg â†’ cat (70.06%)\n","0036.jpg â†’ cat (50.15%)\n","\n","Class: chair\n","0034.jpg â†’ train (47.35%)\n","0006.jpg â†’ cup (62.46%)\n","0029.jpg â†’ cat (33.38%)\n","\n","Class: couch\n","0028.jpg â†’ dog (40.00%)\n","0024.jpg â†’ cat (18.08%)\n","0022.jpg â†’ traffic light (19.56%)\n","\n","Class: cow\n","0030.jpg â†’ person (42.22%)\n","0039.jpg â†’ bird (39.64%)\n","0040.jpg â†’ horse (78.73%)\n","\n","Class: cup\n","0018.jpg â†’ train (17.42%)\n","0046.jpg â†’ person (11.76%)\n","0028.jpg â†’ pizza (36.27%)\n","\n","Class: dog\n","0007.jpg â†’ bird (74.90%)\n","0032.jpg â†’ cat (31.78%)\n","0001.jpg â†’ dog (46.05%)\n","\n","Class: elephant\n","0049.jpg â†’ cat (30.56%)\n","0048.jpg â†’ horse (94.91%)\n","0002.jpg â†’ elephant (94.99%)\n","\n","Class: horse\n","0003.jpg â†’ horse (48.74%)\n","0024.jpg â†’ horse (99.49%)\n","0037.jpg â†’ elephant (18.10%)\n","\n","Class: motorcycle\n","0040.jpg â†’ motorcycle (71.26%)\n","0036.jpg â†’ truck (23.61%)\n","0020.jpg â†’ motorcycle (97.30%)\n","\n","Class: person\n","0049.jpg â†’ traffic light (13.64%)\n","0028.jpg â†’ motorcycle (59.28%)\n","0005.jpg â†’ horse (81.48%)\n","\n","Class: pizza\n","0023.jpg â†’ motorcycle (15.61%)\n","0046.jpg â†’ stop sign (16.86%)\n","0016.jpg â†’ pizza (99.99%)\n","\n","Class: potted plant\n","0028.jpg â†’ bicycle (47.45%)\n","0004.jpg â†’ train (32.46%)\n","0032.jpg â†’ cake (18.40%)\n","\n","Class: stop sign\n","0004.jpg â†’ elephant (21.49%)\n","0033.jpg â†’ stop sign (92.34%)\n","0045.jpg â†’ stop sign (98.23%)\n","\n","Class: traffic light\n","0023.jpg â†’ traffic light (62.71%)\n","0046.jpg â†’ truck (34.73%)\n","0026.jpg â†’ stop sign (16.21%)\n","\n","Class: train\n","0024.jpg â†’ train (99.98%)\n","0034.jpg â†’ train (97.49%)\n","0008.jpg â†’ train (99.98%)\n","\n","Class: truck\n","0011.jpg â†’ truck (36.04%)\n","0003.jpg â†’ motorcycle (29.33%)\n","0043.jpg â†’ bench (27.49%)\n"]}]},{"cell_type":"code","source":["import os\n","import torch\n","import torch.nn as nn\n","from torchvision import models, transforms\n","from PIL import Image\n","\n","class_names = sorted(os.listdir(test_dir))\n","num_classes = len(class_names)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device:\", device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"InkomIeA7Vr_","executionInfo":{"status":"ok","timestamp":1769528451951,"user_tz":-330,"elapsed":9352,"user":{"displayName":"Dilshad Ansari","userId":"01837237404505659019"}},"outputId":"15bde90d-a574-44a6-c5f3-15c5c7eec3c7"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]}]},{"cell_type":"code","source":["model_path = \"/content/drive/MyDrive/SmartVisionAI/models/resnet50_finetuned.pth\"\n","\n","class_names = sorted(os.listdir(test_dir))\n","num_classes = len(class_names)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device:\", device)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TbqRPJjfE0Yh","executionInfo":{"status":"ok","timestamp":1769528451951,"user_tz":-330,"elapsed":2,"user":{"displayName":"Dilshad Ansari","userId":"01837237404505659019"}},"outputId":"fd232a79-05b0-46c4-b2f5-0d2518345aff"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]}]},{"cell_type":"code","source":["model = models.resnet50(weights=None)\n","\n","model.fc = nn.Sequential(\n","    nn.Linear(model.fc.in_features, 512),\n","    nn.ReLU(),\n","    nn.Dropout(0.5),\n","    nn.Linear(512, num_classes)\n",")\n","\n","model.load_state_dict(torch.load(model_path, map_location=device))\n","model = model.to(device)\n","model.eval()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tf60vHC9E9GB","executionInfo":{"status":"ok","timestamp":1769528585591,"user_tz":-330,"elapsed":644,"user":{"displayName":"Dilshad Ansari","userId":"01837237404505659019"}},"outputId":"72ac7a3e-de94-412d-dbf8-e1ba32472859"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu): ReLU(inplace=True)\n","  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  (layer1): Sequential(\n","    (0): Bottleneck(\n","      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): Bottleneck(\n","      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (2): Bottleneck(\n","      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): Bottleneck(\n","      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): Bottleneck(\n","      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (2): Bottleneck(\n","      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (3): Bottleneck(\n","      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): Bottleneck(\n","      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (2): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (3): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (4): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (5): Bottleneck(\n","      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): Bottleneck(\n","      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): Bottleneck(\n","      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","    (2): Bottleneck(\n","      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","    )\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (fc): Sequential(\n","    (0): Linear(in_features=2048, out_features=512, bias=True)\n","    (1): ReLU()\n","    (2): Dropout(p=0.5, inplace=False)\n","    (3): Linear(in_features=512, out_features=26, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(\n","        mean=[0.485, 0.456, 0.406],\n","        std=[0.229, 0.224, 0.225]\n","    )\n","])\n"],"metadata":{"id":"BOMRaWWPHcNU","executionInfo":{"status":"ok","timestamp":1769528589590,"user_tz":-330,"elapsed":2,"user":{"displayName":"Dilshad Ansari","userId":"01837237404505659019"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["print(\"\\n===== Testing ResNet50 (PyTorch) =====\")\n","\n","with torch.no_grad():\n","    for actual_class in class_names:\n","        class_folder = os.path.join(test_dir, actual_class)\n","        images = os.listdir(class_folder)[:3]   # ðŸ‘ˆ ONLY 3 images\n","\n","        print(f\"\\nClass: {actual_class}\")\n","\n","        for img_name in images:\n","            img_path = os.path.join(class_folder, img_name)\n","\n","            img = Image.open(img_path).convert(\"RGB\")\n","            img_tensor = transform(img).unsqueeze(0).to(device)\n","\n","            outputs = model(img_tensor)\n","            probs = torch.softmax(outputs, dim=1)\n","\n","            predicted_idx = torch.argmax(probs, dim=1).item()\n","            predicted_class = class_names[predicted_idx]\n","            confidence = probs[0][predicted_idx].item() * 100\n","\n","            print(\n","                f\"{img_name} â†’ {predicted_class} \"\n","                f\"({confidence:.2f}%)\"\n","            )\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9hQ-MTIRHeKa","executionInfo":{"status":"ok","timestamp":1769528594401,"user_tz":-330,"elapsed":2244,"user":{"displayName":"Dilshad Ansari","userId":"01837237404505659019"}},"outputId":"19e2d4a7-0d13-4859-ec78-3d47ced55158"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","===== Testing ResNet50 (PyTorch) =====\n","\n","Class: airplane\n","0023.jpg â†’ airplane (99.89%)\n","0013.jpg â†’ airplane (99.28%)\n","0031.jpg â†’ airplane (99.78%)\n","\n","Class: bed\n","0042.jpg â†’ bed (92.24%)\n","0045.jpg â†’ couch (52.68%)\n","0024.jpg â†’ bed (78.08%)\n","\n","Class: bench\n","0015.jpg â†’ bench (46.67%)\n","0013.jpg â†’ bench (92.09%)\n","0037.jpg â†’ bench (24.26%)\n","\n","Class: bicycle\n","0036.jpg â†’ bicycle (98.17%)\n","0033.jpg â†’ bottle (19.99%)\n","0031.jpg â†’ bicycle (97.95%)\n","\n","Class: bird\n","0041.jpg â†’ bird (96.58%)\n","0028.jpg â†’ traffic light (85.27%)\n","0003.jpg â†’ bird (99.84%)\n","\n","Class: bottle\n","0032.jpg â†’ bottle (68.29%)\n","0006.jpg â†’ bed (40.24%)\n","0022.jpg â†’ bottle (66.02%)\n","\n","Class: bowl\n","0033.jpg â†’ chair (27.78%)\n","0025.jpg â†’ bowl (81.25%)\n","0004.jpg â†’ bowl (90.28%)\n","\n","Class: bus\n","0007.jpg â†’ truck (33.56%)\n","0048.jpg â†’ bus (96.18%)\n","0025.jpg â†’ potted plant (22.68%)\n","\n","Class: cake\n","0001.jpg â†’ cake (97.20%)\n","0010.jpg â†’ cake (54.22%)\n","0012.jpg â†’ cake (80.65%)\n","\n","Class: car\n","0030.jpg â†’ car (62.54%)\n","0022.jpg â†’ car (57.26%)\n","0028.jpg â†’ car (85.74%)\n","\n","Class: cat\n","0042.jpg â†’ cat (91.52%)\n","0028.jpg â†’ bed (78.39%)\n","0036.jpg â†’ dog (37.47%)\n","\n","Class: chair\n","0034.jpg â†’ chair (54.40%)\n","0006.jpg â†’ chair (50.29%)\n","0029.jpg â†’ chair (64.75%)\n","\n","Class: couch\n","0028.jpg â†’ couch (48.88%)\n","0024.jpg â†’ truck (29.08%)\n","0022.jpg â†’ motorcycle (25.97%)\n","\n","Class: cow\n","0030.jpg â†’ cow (96.19%)\n","0039.jpg â†’ cow (78.69%)\n","0040.jpg â†’ cow (59.78%)\n","\n","Class: cup\n","0018.jpg â†’ cup (58.79%)\n","0046.jpg â†’ couch (18.98%)\n","0028.jpg â†’ bowl (20.86%)\n","\n","Class: dog\n","0007.jpg â†’ dog (64.31%)\n","0032.jpg â†’ dog (71.64%)\n","0001.jpg â†’ dog (97.73%)\n","\n","Class: elephant\n","0049.jpg â†’ bird (85.01%)\n","0048.jpg â†’ elephant (99.68%)\n","0002.jpg â†’ elephant (98.65%)\n","\n","Class: horse\n","0003.jpg â†’ horse (83.02%)\n","0024.jpg â†’ cow (62.94%)\n","0037.jpg â†’ horse (31.00%)\n","\n","Class: motorcycle\n","0040.jpg â†’ motorcycle (99.97%)\n","0036.jpg â†’ motorcycle (94.39%)\n","0020.jpg â†’ motorcycle (99.96%)\n","\n","Class: person\n","0049.jpg â†’ chair (30.83%)\n","0028.jpg â†’ motorcycle (84.09%)\n","0005.jpg â†’ person (97.97%)\n","\n","Class: pizza\n","0023.jpg â†’ truck (27.65%)\n","0046.jpg â†’ pizza (92.08%)\n","0016.jpg â†’ pizza (97.37%)\n","\n","Class: potted plant\n","0028.jpg â†’ potted plant (92.98%)\n","0004.jpg â†’ potted plant (99.46%)\n","0032.jpg â†’ couch (48.86%)\n","\n","Class: stop sign\n","0004.jpg â†’ stop sign (32.24%)\n","0033.jpg â†’ stop sign (99.43%)\n","0045.jpg â†’ stop sign (99.34%)\n","\n","Class: traffic light\n","0023.jpg â†’ traffic light (98.87%)\n","0046.jpg â†’ traffic light (68.47%)\n","0026.jpg â†’ traffic light (81.65%)\n","\n","Class: train\n","0024.jpg â†’ train (99.97%)\n","0034.jpg â†’ train (98.92%)\n","0008.jpg â†’ train (99.97%)\n","\n","Class: truck\n","0011.jpg â†’ truck (59.57%)\n","0003.jpg â†’ truck (90.58%)\n","0043.jpg â†’ car (37.97%)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"X9a27iCUH4YM"},"execution_count":null,"outputs":[]}]}